{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193752e9-ab33-4f1f-b38b-0ab5246bb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from testcontainers.core.container import DockerContainer\n",
    "from testcontainers.core.waiting_utils import wait_for_logs\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5220c112-7452-4eca-8d59-8b4f30d22035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using host tcp://127.0.0.1:65183\n",
      "Pulling image langchain4j/ollama-llama2:latest\n",
      "Container started: cf0562d3136d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ollama = DockerContainer(\"langchain4j/ollama-llama2:latest\").with_kwargs(\n",
    "    device_requests=[docker.types.DeviceRequest(count=-1, capabilities=[['gpu']])]).with_exposed_ports(11434)\n",
    "\n",
    "ollama.start()\n",
    "wait_for_logs(ollama, r\".*Listening on.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c0a541-9c51-484d-b633-0ddbd5ed32a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for container <Container: cf0562d3136d> with image langchain4j/ollama-llama2:latest to be ready ...\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOllama(\n",
    "            temperature=0,\n",
    "            base_url=f'http://{ollama.get_container_host_ip()}:{ollama.get_exposed_port(11434)}',\n",
    "            streaming=True,\n",
    "            # seed=2,\n",
    "            top_k=10,  # A higher value (100) will give more diverse answers, while a lower value (10) will be more conservative.\n",
    "            top_p=0.3,  # Higher value (0.95) will lead to more diverse text, while a lower value (0.5) will generate more focused text.\n",
    "            num_ctx=3072,  # Sets the size of the context window used to generate the next token.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c986e491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm just an AI, I don't have feelings or emotions like humans do, so I can't experience the world in the same way that you do. However, I'm here to help you with any questions or tasks you may have, so feel free to ask me anything!\n"
     ]
    }
   ],
   "source": [
    "message = chat.invoke(\"How are you?\")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6de51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
